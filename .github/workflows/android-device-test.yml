name: android-device-test

on:
  workflow_run:
    workflows: [android-ci]
    types:
      - completed

jobs:
  pre_job:
    runs-on: ubuntu-latest
    outputs:
      should_skip: ${{ steps.check_skip.outputs.should_skip }}
    steps:
      # if android-build in android-ci was skipped, the android-device-test job can be skipped entirely
      - id: check_skip
        run: |
          conclusion=$(curl ${{ github.event.workflow_run.jobs_url }} | jq -r '.jobs[] | select(.name == "android-build").conclusion')
          should_skip=$([ "$conclusion" = "skipped" ] && echo "true" || echo "false")
          echo "should_skip=$should_skip" >> "$GITHUB_OUTPUT"

  android-device-test:
    needs: pre_job
    if: needs.pre_job.outputs.should_skip == 'false'
    strategy:
      max-parallel: 2
      matrix:
        test: [
          {
            artifactName: android-render-tests,
            testFile: RenderTests.apk,
            appFile: RenderTestsApp.apk,
            name: "Android Render Tests",
            # Google Pixel 7 Pro
            devicePool: "arn:aws:devicefarm:us-west-2:373521797162:devicepool:20687d72-0e46-403e-8f03-0941850665bc/9692fe7f-86a9-4ecc-908f-175600968564"
          },
          {
            artifactName: benchmarkAPKs,
            testFile: "MapboxGLAndroidSDKTestApp-drawable-release-androidTest.apk",
            appFile: "MapboxGLAndroidSDKTestApp-drawable-release.apk",
            name: "Android Benchmark",
            testFilter: "org.maplibre.android.benchmark.Benchmark",
            # top devices, query with `aws list-device-pools --arn <project_arn>`
            devicePool: "arn:aws:devicefarm:us-west-2::devicepool:082d10e5-d7d7-48a5-ba5c-b33d66efa1f5",
            # benchmark-android.yaml
            # see https://github.com/maplibre/ci-runners/tree/main/aws-device-farm/custom-test-envs
            testSpecArn: "arn:aws:devicefarm:us-west-2:373521797162:upload:20687d72-0e46-403e-8f03-0941850665bc/14862afb-cf88-44aa-9f1e-5131cbb22f01"
          }
        ]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Sanity check, test files should exist
        uses: andstor/file-existence-action@v2.0.0
        with:
          files: "${{ matrix.test.testFile }}, ${{ matrix.test.appFile }}"
          fail: true

      # get comment from PR

      - uses: ./.github/actions/get-pr-number
        id: get-pr-number

      - name: Generate token
        if: matrix.test.name == 'Android Benchmark' && steps.get-pr-number.outputs.pr-number
        id: generate_token
        uses: tibdex/github-app-token@v2
        with:
          app_id: ${{ secrets.MAPLIBRE_NATIVE_BOT_APP_ID }}
          private_key: ${{ secrets.MAPLIBRE_NATIVE_BOT_PRIVATE_KEY }}

      - name: Check if comment on PR contains '!benchmark android'
        if: matrix.test.name == 'Android Benchmark' && steps.get-pr-number.outputs.pr-number
        uses: peter-evans/find-comment@v3
        id: benchmark_comment
        with:
          issue-number: ${{ steps.get-pr-number.outputs.pr-number }}
          body-regex: '^!benchmark.*android.*$'

      - name: Should we run this device test?
        # always run when something was merged into main
        # run benchmark when comment with '!benchmark android' exists in PR
        if: |
          (github.event.workflow_run.head_branch == 'main' && github.event.workflow_run.event == 'push') ||
          matrix.test.name == 'Android Benchmark' && steps.benchmark_comment.outputs.comment-id ||
          matrix.test.name != 'Android Benchmark'
        run: 
          echo "run_device_test=true" >> "$GITHUB_ENV"

      - uses: LouisBrunner/checks-action@v2.0.0
        if: env.run_device_test == 'true'
        id: create_check
        with:
          token: ${{ steps.generate_token.outputs.token }}
          details_url: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          status: queued
          name: ${{ matrix.test.name }}
          sha: ${{ github.event.workflow_run.head_sha }}

      - uses: ./.github/actions/download-workflow-run-artifact
        if: env.run_device_test == 'true'
        with:
          artifact-name: ${{ matrix.test.artifactName }}

      - name: Configure AWS Credentials
        if: env.run_device_test == 'true' && matrix.test.name == 'Android Benchmark'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2
          role-to-assume: ${{ vars.AWS_ROLE_TO_ASSUME }}
          role-duration-seconds: 21600
          role-session-name: MySessionName

      - name: Upload external data for benchmark
        if: env.run_device_test == 'true' && matrix.test.name == 'Android Benchmark'
        run: |
          export RESULTS_API=${{ secrets.MLN_RESULTS_API }}
          export AWS_DEVICE_FARM_PROJECT_ARN=${{ vars.AWS_DEVICE_FARM_PROJECT_ARN }}
          upload_arn="$(.github/workflows/android-device-test/upload-external-data.sh)"
          echo external_data_arn="$upload_arn" >> "$GITHUB_ENV"

      - uses: ./.github/actions/aws-device-farm-run
        if: env.run_device_test == 'true'
        with:
          name: ${{ matrix.test.name }}
          appType: ANDROID_APP
          appFile: ${{ matrix.test.appFile }}
          testFile: ${{ matrix.test.testFile }}
          testPackageType: INSTRUMENTATION_TEST_PACKAGE
          testType: INSTRUMENTATION
          testFilter: ${{ matrix.test.testFilter }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_ROLE_TO_ASSUME: ${{ vars.AWS_ROLE_TO_ASSUME }}
          AWS_DEVICE_FARM_PROJECT_ARN: ${{ vars.AWS_DEVICE_FARM_PROJECT_ARN }}
          AWS_DEVICE_FARM_DEVICE_POOL_ARN: ${{ matrix.test.devicePool }}
          externalData: ${{ env.external_data_arn }}
          testSpecArn: ${{ matrix.test.testSpecArn }}

      - uses: LouisBrunner/checks-action@v2.0.0
        if: always() && env.run_device_test == 'true'
        with:
          token: ${{ steps.generate_token.outputs.token }}
          check_id: ${{ steps.create_check.outputs.check_id }}
          conclusion: ${{ job.status }}
          sha: ${{ github.event.workflow_run.sha }}
